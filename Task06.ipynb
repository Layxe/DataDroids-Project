{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7511757c26d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data/tweeteval/emotion/train_text.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/train_labels.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/test_text.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n",
      "Loading ./data/tweeteval/emotion/test_labels.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n"
     ]
    }
   ],
   "source": [
    "EMOTION_LABEL = {\n",
    "    \"anger\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"optimism\": 2,\n",
    "    \"sadness\": 3\n",
    "}\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, training):\n",
    "        spacy.prefer_gpu()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.data = None\n",
    "\n",
    "        if training:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/train_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/train_labels.txt\"\n",
    "        else:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/test_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/test_labels.txt\"\n",
    "\n",
    "        self.data   = self._load_txt_file(self.path_text)\n",
    "        self.labels = self._load_txt_file(self.path_labels, perform_nlp=False)\n",
    "\n",
    "    def _load_txt_file(self, path, perform_nlp=True):\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            print(f\"Loading {path} with {len(lines)} lines\")\n",
    "\n",
    "            for i in range(len(lines)):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Processing line {i}/{len(lines)}\")\n",
    "\n",
    "                lines[i] = lines[i].strip()\n",
    "                lines[i] = lines[i].replace(\"\\n\", \"\")\n",
    "\n",
    "                if perform_nlp:\n",
    "                    lines[i] = self.nlp(lines[i])\n",
    "\n",
    "            return lines\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = EmotionDataset(training=True)\n",
    "test_dataset  = EmotionDataset(training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 85\n"
     ]
    }
   ],
   "source": [
    "word_to_ix  = {}\n",
    "word_counts = {}\n",
    "\n",
    "maximum_length = 0\n",
    "\n",
    "# Count the occurrences of each word\n",
    "for sent, tags in train_dataset:\n",
    "\n",
    "    if len(sent) > maximum_length:\n",
    "        maximum_length = len(sent)\n",
    "\n",
    "    for word in sent:\n",
    "        if word.text not in word_counts:\n",
    "            word_counts[word.text] = 1\n",
    "        else:\n",
    "            word_counts[word.text] += 1\n",
    "\n",
    "print(f\"Maximum length: {maximum_length}\")\n",
    "\n",
    "# Sort words by counts\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Assign ID to the most 5000 frequent words\n",
    "for word, _ in sorted_word_counts[100:5100]:\n",
    "    word_to_ix[word] = len(word_to_ix) + 1\n",
    "\n",
    "def get_idx(word):\n",
    "    if word in word_to_ix:\n",
    "        return word_to_ix[word]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = [get_idx(w) for w in seq]\n",
    "\n",
    "    template_tensor = torch.zeros(maximum_length, dtype=torch.long)\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        template_tensor[i] = idx\n",
    "\n",
    "    return template_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 316, 2301,    0,    0,   82, 3965,    0,    0,  389,    0,  317,   46,\n",
       "            0,    0,    0, 1293, 3966, 3967,    0,    0,    0, 1677,    0,  905,\n",
       "            0,  149,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0]),\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmotionTensorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data    = []\n",
    "\n",
    "        for sentence, label in self.dataset:\n",
    "            self.data.append((prepare_sequence([word.text for word in sentence]), label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.data[index][0]\n",
    "        label     = self.data[index][1]\n",
    "\n",
    "        return {'input_ids': input_ids, 'label': int(label)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_tensor_dataset = EmotionTensorDataset(train_dataset)\n",
    "test_tensor_dataset  = EmotionTensorDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1523,  4737,  2003,  1037,  2091,  7909,  2006,  1037,  3291,\n",
       "         2017,  2089,  2196,  2031,  1005,  1012, 11830, 11527,  1012,  1001,\n",
       "        14354,  1001,  4105,  1001,  4737,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'label': 2}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", truncation=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class EmotionBertDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset   = dataset\n",
    "\n",
    "        self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.data = []\n",
    "\n",
    "        for sentence, label in self.dataset:\n",
    "\n",
    "            dict_input = tokenizer(sentence.text, padding='max_length', max_length=maximum_length, return_tensors=\"pt\")\n",
    "\n",
    "            dict_input[\"input_ids\"] = dict_input[\"input_ids\"].squeeze(0)\n",
    "            dict_input[\"attention_mask\"] = dict_input[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "            dict_input[\"label\"] = int(label)\n",
    "\n",
    "            self.data.append(dict_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_bert_dataset = EmotionBertDataset(train_dataset)\n",
    "test_bert_dataset  = EmotionBertDataset(test_dataset)\n",
    "\n",
    "train_bert_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0: \"anger\", 1: \"joy\", 2: \"optimism\", 3: \"sadness\"}\n",
    "label2id = {\"anger\": 0, \"joy\": 1, \"optimism\": 2, \"sadness\": 3}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=4, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 16:36, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.615939</td>\n",
       "      <td>0.788177</td>\n",
       "      <td>0.702175</td>\n",
       "      <td>0.805829</td>\n",
       "      <td>0.688677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631836</td>\n",
       "      <td>0.790289</td>\n",
       "      <td>0.735336</td>\n",
       "      <td>0.798930</td>\n",
       "      <td>0.711815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754719</td>\n",
       "      <td>0.784659</td>\n",
       "      <td>0.751411</td>\n",
       "      <td>0.764824</td>\n",
       "      <td>0.746746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.898285</td>\n",
       "      <td>0.781844</td>\n",
       "      <td>0.747633</td>\n",
       "      <td>0.764309</td>\n",
       "      <td>0.741684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.234415</td>\n",
       "      <td>0.771288</td>\n",
       "      <td>0.723272</td>\n",
       "      <td>0.773087</td>\n",
       "      <td>0.698760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.030157</td>\n",
       "      <td>0.794511</td>\n",
       "      <td>0.752310</td>\n",
       "      <td>0.777852</td>\n",
       "      <td>0.740311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.205954</td>\n",
       "      <td>0.786770</td>\n",
       "      <td>0.753967</td>\n",
       "      <td>0.746361</td>\n",
       "      <td>0.764002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.347947</td>\n",
       "      <td>0.787474</td>\n",
       "      <td>0.749131</td>\n",
       "      <td>0.762742</td>\n",
       "      <td>0.741929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.362073</td>\n",
       "      <td>0.786770</td>\n",
       "      <td>0.752075</td>\n",
       "      <td>0.768892</td>\n",
       "      <td>0.741990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>1.677583</td>\n",
       "      <td>0.766362</td>\n",
       "      <td>0.723693</td>\n",
       "      <td>0.743843</td>\n",
       "      <td>0.712493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>1.538662</td>\n",
       "      <td>0.776214</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>0.758193</td>\n",
       "      <td>0.717954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>1.502304</td>\n",
       "      <td>0.779029</td>\n",
       "      <td>0.743352</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.744569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>1.564617</td>\n",
       "      <td>0.786066</td>\n",
       "      <td>0.744738</td>\n",
       "      <td>0.761704</td>\n",
       "      <td>0.733483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>1.716070</td>\n",
       "      <td>0.781140</td>\n",
       "      <td>0.735970</td>\n",
       "      <td>0.746797</td>\n",
       "      <td>0.728876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.712873</td>\n",
       "      <td>0.784659</td>\n",
       "      <td>0.737804</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.728610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.638181</td>\n",
       "      <td>0.786770</td>\n",
       "      <td>0.747553</td>\n",
       "      <td>0.748872</td>\n",
       "      <td>0.747964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.772814</td>\n",
       "      <td>0.774806</td>\n",
       "      <td>0.739041</td>\n",
       "      <td>0.762373</td>\n",
       "      <td>0.724008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.649297</td>\n",
       "      <td>0.793807</td>\n",
       "      <td>0.763445</td>\n",
       "      <td>0.766028</td>\n",
       "      <td>0.761525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>1.688861</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.766400</td>\n",
       "      <td>0.768271</td>\n",
       "      <td>0.765236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.787013</td>\n",
       "      <td>0.785362</td>\n",
       "      <td>0.747032</td>\n",
       "      <td>0.756603</td>\n",
       "      <td>0.741437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.750589</td>\n",
       "      <td>0.788881</td>\n",
       "      <td>0.755755</td>\n",
       "      <td>0.756797</td>\n",
       "      <td>0.755084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.786810</td>\n",
       "      <td>0.790992</td>\n",
       "      <td>0.754586</td>\n",
       "      <td>0.762339</td>\n",
       "      <td>0.749199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.791098</td>\n",
       "      <td>0.790992</td>\n",
       "      <td>0.758342</td>\n",
       "      <td>0.760102</td>\n",
       "      <td>0.757424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>1.814848</td>\n",
       "      <td>0.784659</td>\n",
       "      <td>0.749552</td>\n",
       "      <td>0.752820</td>\n",
       "      <td>0.748626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.774542</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.755829</td>\n",
       "      <td>0.761634</td>\n",
       "      <td>0.751566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.784419</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.756794</td>\n",
       "      <td>0.764057</td>\n",
       "      <td>0.751360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.791144</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.756828</td>\n",
       "      <td>0.764149</td>\n",
       "      <td>0.751360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.796319</td>\n",
       "      <td>0.791696</td>\n",
       "      <td>0.755686</td>\n",
       "      <td>0.762398</td>\n",
       "      <td>0.750661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.800422</td>\n",
       "      <td>0.791696</td>\n",
       "      <td>0.756165</td>\n",
       "      <td>0.763549</td>\n",
       "      <td>0.750661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.801675</td>\n",
       "      <td>0.791696</td>\n",
       "      <td>0.756165</td>\n",
       "      <td>0.763549</td>\n",
       "      <td>0.750661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3060, training_loss=0.0683016069355159, metrics={'train_runtime': 996.951, 'train_samples_per_second': 98.009, 'train_steps_per_second': 3.069, 'total_flos': 2148881542995600.0, 'train_loss': 0.0683016069355159, 'epoch': 30.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_bert_dataset,\n",
    "    eval_dataset=test_bert_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3060' max='3060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3060/3060 17:31, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.292542</td>\n",
       "      <td>0.392681</td>\n",
       "      <td>0.140980</td>\n",
       "      <td>0.098170</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.304042</td>\n",
       "      <td>0.392681</td>\n",
       "      <td>0.140980</td>\n",
       "      <td>0.098170</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.288129</td>\n",
       "      <td>0.392681</td>\n",
       "      <td>0.140980</td>\n",
       "      <td>0.098170</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.294428</td>\n",
       "      <td>0.395496</td>\n",
       "      <td>0.147274</td>\n",
       "      <td>0.154985</td>\n",
       "      <td>0.252618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.336082</td>\n",
       "      <td>0.314567</td>\n",
       "      <td>0.180018</td>\n",
       "      <td>0.193671</td>\n",
       "      <td>0.267562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.345885</td>\n",
       "      <td>0.313160</td>\n",
       "      <td>0.176735</td>\n",
       "      <td>0.199547</td>\n",
       "      <td>0.268524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.320340</td>\n",
       "      <td>0.325827</td>\n",
       "      <td>0.192744</td>\n",
       "      <td>0.192052</td>\n",
       "      <td>0.270396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.372412</td>\n",
       "      <td>0.374384</td>\n",
       "      <td>0.229429</td>\n",
       "      <td>0.342014</td>\n",
       "      <td>0.301810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.243700</td>\n",
       "      <td>1.295830</td>\n",
       "      <td>0.460943</td>\n",
       "      <td>0.277697</td>\n",
       "      <td>0.285423</td>\n",
       "      <td>0.348824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.217484</td>\n",
       "      <td>0.479944</td>\n",
       "      <td>0.303855</td>\n",
       "      <td>0.294215</td>\n",
       "      <td>0.360828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.344164</td>\n",
       "      <td>0.472906</td>\n",
       "      <td>0.371603</td>\n",
       "      <td>0.382446</td>\n",
       "      <td>0.386938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.191937</td>\n",
       "      <td>0.506685</td>\n",
       "      <td>0.441255</td>\n",
       "      <td>0.478020</td>\n",
       "      <td>0.450236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.305295</td>\n",
       "      <td>0.514426</td>\n",
       "      <td>0.437514</td>\n",
       "      <td>0.477241</td>\n",
       "      <td>0.444630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.080800</td>\n",
       "      <td>1.488531</td>\n",
       "      <td>0.519353</td>\n",
       "      <td>0.461342</td>\n",
       "      <td>0.478195</td>\n",
       "      <td>0.466730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>1.577114</td>\n",
       "      <td>0.504574</td>\n",
       "      <td>0.439035</td>\n",
       "      <td>0.461859</td>\n",
       "      <td>0.440056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>1.577823</td>\n",
       "      <td>0.486981</td>\n",
       "      <td>0.445458</td>\n",
       "      <td>0.491541</td>\n",
       "      <td>0.465851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>1.670812</td>\n",
       "      <td>0.503871</td>\n",
       "      <td>0.456032</td>\n",
       "      <td>0.480870</td>\n",
       "      <td>0.466264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>1.683894</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>0.459178</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.472252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>1.792674</td>\n",
       "      <td>0.525686</td>\n",
       "      <td>0.451887</td>\n",
       "      <td>0.467897</td>\n",
       "      <td>0.453392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.909608</td>\n",
       "      <td>0.496833</td>\n",
       "      <td>0.446394</td>\n",
       "      <td>0.462810</td>\n",
       "      <td>0.454760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.874521</td>\n",
       "      <td>0.512315</td>\n",
       "      <td>0.460697</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.471021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.809046</td>\n",
       "      <td>0.494018</td>\n",
       "      <td>0.453813</td>\n",
       "      <td>0.479506</td>\n",
       "      <td>0.473525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.893831</td>\n",
       "      <td>0.504574</td>\n",
       "      <td>0.445330</td>\n",
       "      <td>0.453074</td>\n",
       "      <td>0.450550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>2.030302</td>\n",
       "      <td>0.506685</td>\n",
       "      <td>0.451642</td>\n",
       "      <td>0.463619</td>\n",
       "      <td>0.458957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>1.968516</td>\n",
       "      <td>0.517945</td>\n",
       "      <td>0.451936</td>\n",
       "      <td>0.454549</td>\n",
       "      <td>0.454549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>2.047464</td>\n",
       "      <td>0.517945</td>\n",
       "      <td>0.453027</td>\n",
       "      <td>0.455047</td>\n",
       "      <td>0.455958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>2.061988</td>\n",
       "      <td>0.513019</td>\n",
       "      <td>0.445281</td>\n",
       "      <td>0.447404</td>\n",
       "      <td>0.447374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>2.105963</td>\n",
       "      <td>0.513019</td>\n",
       "      <td>0.438811</td>\n",
       "      <td>0.439185</td>\n",
       "      <td>0.439741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.257200</td>\n",
       "      <td>2.134221</td>\n",
       "      <td>0.510908</td>\n",
       "      <td>0.448480</td>\n",
       "      <td>0.451902</td>\n",
       "      <td>0.451798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>2.131926</td>\n",
       "      <td>0.510908</td>\n",
       "      <td>0.446935</td>\n",
       "      <td>0.448245</td>\n",
       "      <td>0.449906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/alex/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3060, training_loss=0.6195948678683612, metrics={'train_runtime': 1052.0671, 'train_samples_per_second': 92.874, 'train_steps_per_second': 2.909, 'total_flos': 2148881542995600.0, 'train_loss': 0.6195948678683612, 'epoch': 30.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_prev = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tensor_dataset,\n",
    "    eval_dataset=test_tensor_dataset,\n",
    "    tokenizer=None,\n",
    "    data_collator=None,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_prev.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBert with preprocessing from A5\n",
    "\n",
    "Accuracy: ~51.8%\n",
    "\n",
    "#### DistilBert with Bert Preprocessing\n",
    "\n",
    "Accuracy: ~79.2%\n",
    "\n",
    "#### LSTM\n",
    "\n",
    "Accuracy: ~61.3%\n",
    "\n",
    "#### GRU\n",
    "\n",
    "Accuracy: ~59.75%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
