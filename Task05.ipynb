{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bbce41c66f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data/tweeteval/emotion/train_text.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/train_labels.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/test_text.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n",
      "Loading ./data/tweeteval/emotion/test_labels.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n"
     ]
    }
   ],
   "source": [
    "EMOTION_LABEL = {\n",
    "    \"anger\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"optimism\": 2,\n",
    "    \"sadness\": 3\n",
    "}\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, training):\n",
    "        spacy.prefer_gpu()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.data = None\n",
    "\n",
    "        if training:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/train_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/train_labels.txt\"\n",
    "        else:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/test_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/test_labels.txt\"\n",
    "\n",
    "        self.data   = self._load_txt_file(self.path_text)\n",
    "        self.labels = self._load_txt_file(self.path_labels, perform_nlp=False)\n",
    "\n",
    "    def _load_txt_file(self, path, perform_nlp=True):\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            print(f\"Loading {path} with {len(lines)} lines\")\n",
    "\n",
    "            for i in range(len(lines)):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Processing line {i}/{len(lines)}\")\n",
    "\n",
    "                lines[i] = lines[i].strip()\n",
    "                lines[i] = lines[i].replace(\"\\n\", \"\")\n",
    "\n",
    "                if perform_nlp:\n",
    "                    lines[i] = self.nlp(lines[i])\n",
    "\n",
    "            return lines\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], int(self.labels[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = EmotionDataset(training=True)\n",
    "test_dataset  = EmotionDataset(training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256, vocab_size=5001, tagset_size=4):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 85, tagset_size)\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        batch_size = len(sentence)\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = nn.utils.rnn.pack_padded_sequence(embeds, [85 for i in range(sentence.shape[0])], batch_first=True)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out_unpacked, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        # tag_space = self.hidden2tag(out_unpacked.view(batch_size, sentence.shape[1], -1))\n",
    "        out_unpacked = out_unpacked.reshape(batch_size, -1)\n",
    "        tag_space = self.hidden2tag(out_unpacked)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 85\n"
     ]
    }
   ],
   "source": [
    "word_to_ix  = {}\n",
    "word_counts = {}\n",
    "\n",
    "maximum_length = 0\n",
    "\n",
    "# Count the occurrences of each word\n",
    "for sent, tags in train_dataset:\n",
    "\n",
    "    if len(sent) > maximum_length:\n",
    "        maximum_length = len(sent)\n",
    "\n",
    "    for word in sent:\n",
    "        if word.text not in word_counts:\n",
    "            word_counts[word.text] = 1\n",
    "        else:\n",
    "            word_counts[word.text] += 1\n",
    "\n",
    "print(f\"Maximum length: {maximum_length}\")\n",
    "\n",
    "# Sort words by counts\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Assign ID to the most 5000 frequent words\n",
    "for word, _ in sorted_word_counts[100:5100]:\n",
    "    word_to_ix[word] = len(word_to_ix) + 1\n",
    "\n",
    "def get_idx(word):\n",
    "    if word in word_to_ix:\n",
    "        return word_to_ix[word]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    idxs = [get_idx(w) for w in seq]\n",
    "\n",
    "    template_tensor = torch.zeros(maximum_length, dtype=torch.long)\n",
    "\n",
    "    for i, idx in enumerate(idxs):\n",
    "        template_tensor[i] = idx\n",
    "\n",
    "    return template_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTensorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data    = []\n",
    "\n",
    "        for sentence, label in self.dataset:\n",
    "            self.data.append((prepare_sequence([word.text for word in sentence]), label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_tensor_dataset = EmotionTensorDataset(train_dataset)\n",
    "test_tensor_dataset  = EmotionTensorDataset(test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_tensor_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0, loss: 1.3759361505508423\n",
      "Epoch 0, accuracy: 0.41379310344827586\n",
      "Epoch 1, iteration 0, loss: 1.0314100980758667\n",
      "Epoch 1, accuracy: 0.4644616467276566\n",
      "Epoch 2, iteration 0, loss: 0.927951455116272\n",
      "Epoch 2, accuracy: 0.48909218859957776\n",
      "Epoch 3, iteration 0, loss: 0.7023993730545044\n",
      "Epoch 3, accuracy: 0.525686136523575\n",
      "Epoch 4, iteration 0, loss: 0.3833940625190735\n",
      "Epoch 4, accuracy: 0.5446868402533427\n",
      "Epoch 5, iteration 0, loss: 0.2607744634151459\n",
      "Epoch 5, accuracy: 0.5453905700211119\n",
      "Epoch 6, iteration 0, loss: 0.1475040465593338\n",
      "Epoch 6, accuracy: 0.5601688951442646\n",
      "Epoch 7, iteration 0, loss: 0.08462411165237427\n",
      "Epoch 7, accuracy: 0.5679099225897255\n",
      "Epoch 8, iteration 0, loss: 0.0878586620092392\n",
      "Epoch 8, accuracy: 0.5721323011963406\n",
      "Epoch 9, iteration 0, loss: 0.01772087626159191\n",
      "Epoch 9, accuracy: 0.5608726249120338\n",
      "Epoch 10, iteration 0, loss: 0.00876993965357542\n",
      "Epoch 10, accuracy: 0.5742434904996482\n",
      "Epoch 11, iteration 0, loss: 0.005613474640995264\n",
      "Epoch 11, accuracy: 0.5573539760731879\n",
      "Epoch 12, iteration 0, loss: 0.005107213743031025\n",
      "Epoch 12, accuracy: 0.5707248416608023\n",
      "Epoch 13, iteration 0, loss: 0.003362565068528056\n",
      "Epoch 13, accuracy: 0.5798733286418015\n",
      "Epoch 14, iteration 0, loss: 0.033921342343091965\n",
      "Epoch 14, accuracy: 0.5742434904996482\n",
      "Epoch 15, iteration 0, loss: 0.0030667309183627367\n",
      "Epoch 15, accuracy: 0.5756509500351865\n",
      "Epoch 16, iteration 0, loss: 0.0020332818385213614\n",
      "Epoch 16, accuracy: 0.5749472202674173\n",
      "Epoch 17, iteration 0, loss: 0.0015045871259644628\n",
      "Epoch 17, accuracy: 0.565798733286418\n",
      "Epoch 18, iteration 0, loss: 0.0009080572635866702\n",
      "Epoch 18, accuracy: 0.5728360309641097\n",
      "Epoch 19, iteration 0, loss: 0.0014972011558711529\n",
      "Epoch 19, accuracy: 0.5700211118930331\n",
      "Epoch 20, iteration 0, loss: 0.00041679077548906207\n",
      "Epoch 20, accuracy: 0.5728360309641097\n",
      "Epoch 21, iteration 0, loss: 0.0004676557728089392\n",
      "Epoch 21, accuracy: 0.5643912737508796\n",
      "Epoch 22, iteration 0, loss: 0.00023487261205445975\n",
      "Epoch 22, accuracy: 0.5643912737508796\n",
      "Epoch 23, iteration 0, loss: 0.00030225422233343124\n",
      "Epoch 23, accuracy: 0.565798733286418\n",
      "Epoch 24, iteration 0, loss: 0.00019908718240913004\n",
      "Epoch 24, accuracy: 0.5643912737508796\n",
      "Epoch 25, iteration 0, loss: 0.00016608939040452242\n",
      "Epoch 25, accuracy: 0.5643912737508796\n",
      "Epoch 26, iteration 0, loss: 0.0003530184621922672\n",
      "Epoch 26, accuracy: 0.5636875439831105\n",
      "Epoch 27, iteration 0, loss: 0.00019227179291192442\n",
      "Epoch 27, accuracy: 0.5615763546798029\n",
      "Epoch 28, iteration 0, loss: 8.544678712496534e-05\n",
      "Epoch 28, accuracy: 0.5650950035186488\n",
      "Epoch 29, iteration 0, loss: 0.00010131920862477273\n",
      "Epoch 29, accuracy: 0.565798733286418\n",
      "Epoch 30, iteration 0, loss: 8.340958447661251e-05\n",
      "Epoch 30, accuracy: 0.5622800844475722\n",
      "Epoch 31, iteration 0, loss: 0.00020118808606639504\n",
      "Epoch 31, accuracy: 0.5622800844475722\n",
      "Epoch 32, iteration 0, loss: 8.673778938828036e-05\n",
      "Epoch 32, accuracy: 0.5629838142153413\n",
      "Epoch 33, iteration 0, loss: 0.00010416738950880244\n",
      "Epoch 33, accuracy: 0.5650950035186488\n",
      "Epoch 34, iteration 0, loss: 8.867769793141633e-05\n",
      "Epoch 34, accuracy: 0.5608726249120338\n",
      "Epoch 35, iteration 0, loss: 4.163493576925248e-05\n",
      "Epoch 35, accuracy: 0.5636875439831105\n",
      "Epoch 36, iteration 0, loss: 9.593510185368359e-05\n",
      "Epoch 36, accuracy: 0.5601688951442646\n",
      "Epoch 37, iteration 0, loss: 2.5110281058005057e-05\n",
      "Epoch 37, accuracy: 0.5636875439831105\n",
      "Epoch 38, iteration 0, loss: 2.595713158370927e-05\n",
      "Epoch 38, accuracy: 0.5573539760731879\n",
      "Epoch 39, iteration 0, loss: 8.903642446966842e-05\n",
      "Epoch 39, accuracy: 0.5601688951442646\n",
      "Epoch 40, iteration 0, loss: 2.975104325741995e-05\n",
      "Epoch 40, accuracy: 0.5601688951442646\n",
      "Epoch 41, iteration 0, loss: 2.6781890483107418e-05\n",
      "Epoch 41, accuracy: 0.5608726249120338\n",
      "Epoch 42, iteration 0, loss: 2.3052783944876865e-05\n",
      "Epoch 42, accuracy: 0.5601688951442646\n",
      "Epoch 43, iteration 0, loss: 2.637508987390902e-05\n",
      "Epoch 43, accuracy: 0.5622800844475722\n",
      "Epoch 44, iteration 0, loss: 2.9845952667528763e-05\n",
      "Epoch 44, accuracy: 0.5622800844475722\n",
      "Epoch 45, iteration 0, loss: 1.60286326718051e-05\n",
      "Epoch 45, accuracy: 0.5622800844475722\n",
      "Epoch 46, iteration 0, loss: 1.688592237769626e-05\n",
      "Epoch 46, accuracy: 0.5608726249120338\n",
      "Epoch 47, iteration 0, loss: 2.2459045794676058e-05\n",
      "Epoch 47, accuracy: 0.5608726249120338\n",
      "Epoch 48, iteration 0, loss: 1.441236054233741e-05\n",
      "Epoch 48, accuracy: 0.5594651653764954\n",
      "Epoch 49, iteration 0, loss: 2.2817081116954796e-05\n",
      "Epoch 49, accuracy: 0.5608726249120338\n",
      "Epoch 50, iteration 0, loss: 1.5818739484529942e-05\n",
      "Epoch 50, accuracy: 0.5594651653764954\n",
      "Epoch 51, iteration 0, loss: 1.2353784768492915e-05\n",
      "Epoch 51, accuracy: 0.5594651653764954\n",
      "Epoch 52, iteration 0, loss: 1.1788293704739772e-05\n",
      "Epoch 52, accuracy: 0.5587614356087263\n",
      "Epoch 53, iteration 0, loss: 1.9809522200375795e-05\n",
      "Epoch 53, accuracy: 0.5594651653764954\n",
      "Epoch 54, iteration 0, loss: 1.1154640560562257e-05\n",
      "Epoch 54, accuracy: 0.5587614356087263\n",
      "Epoch 55, iteration 0, loss: 9.219598723575473e-06\n",
      "Epoch 55, accuracy: 0.5587614356087263\n",
      "Epoch 56, iteration 0, loss: 1.3162858522264287e-05\n",
      "Epoch 56, accuracy: 0.5587614356087263\n",
      "Epoch 57, iteration 0, loss: 9.741304893395863e-06\n",
      "Epoch 57, accuracy: 0.5594651653764954\n",
      "Epoch 58, iteration 0, loss: 9.491565833741333e-06\n",
      "Epoch 58, accuracy: 0.5587614356087263\n",
      "Epoch 59, iteration 0, loss: 3.5912780731450766e-05\n",
      "Epoch 59, accuracy: 0.5587614356087263\n",
      "Epoch 60, iteration 0, loss: 1.0173452210437972e-05\n",
      "Epoch 60, accuracy: 0.558057705840957\n",
      "Epoch 61, iteration 0, loss: 5.250595222605625e-06\n",
      "Epoch 61, accuracy: 0.5573539760731879\n",
      "Epoch 62, iteration 0, loss: 7.079729130055057e-06\n",
      "Epoch 62, accuracy: 0.558057705840957\n",
      "Epoch 63, iteration 0, loss: 2.0091456462978385e-05\n",
      "Epoch 63, accuracy: 0.5587614356087263\n",
      "Epoch 64, iteration 0, loss: 4.302660727262264e-06\n",
      "Epoch 64, accuracy: 0.5601688951442646\n",
      "Epoch 65, iteration 0, loss: 8.107174835458864e-06\n",
      "Epoch 65, accuracy: 0.5587614356087263\n",
      "Epoch 66, iteration 0, loss: 1.0584946721792221e-05\n",
      "Epoch 66, accuracy: 0.5587614356087263\n",
      "Epoch 67, iteration 0, loss: 3.563181735444232e-06\n",
      "Epoch 67, accuracy: 0.5587614356087263\n",
      "Epoch 68, iteration 0, loss: 4.7924818318278994e-06\n",
      "Epoch 68, accuracy: 0.558057705840957\n",
      "Epoch 69, iteration 0, loss: 2.8963288514205487e-06\n",
      "Epoch 69, accuracy: 0.5552427867698804\n",
      "Epoch 70, iteration 0, loss: 6.859675522719044e-06\n",
      "Epoch 70, accuracy: 0.5601688951442646\n",
      "Epoch 71, iteration 0, loss: 1.681955609456054e-06\n",
      "Epoch 71, accuracy: 0.5566502463054187\n",
      "Epoch 72, iteration 0, loss: 2.9559844278992387e-06\n",
      "Epoch 72, accuracy: 0.558057705840957\n",
      "Epoch 73, iteration 0, loss: 3.3508586056996137e-06\n",
      "Epoch 73, accuracy: 0.5552427867698804\n",
      "Epoch 74, iteration 0, loss: 1.6949891232798109e-06\n",
      "Epoch 74, accuracy: 0.5545390570021111\n",
      "Epoch 75, iteration 0, loss: 2.695228204174782e-06\n",
      "Epoch 75, accuracy: 0.5559465165376496\n",
      "Epoch 76, iteration 0, loss: 4.779305527335964e-06\n",
      "Epoch 76, accuracy: 0.5552427867698804\n",
      "Epoch 77, iteration 0, loss: 3.788574758800678e-06\n",
      "Epoch 77, accuracy: 0.5552427867698804\n",
      "Epoch 78, iteration 0, loss: 2.797659362840932e-06\n",
      "Epoch 78, accuracy: 0.558057705840957\n",
      "Epoch 79, iteration 0, loss: 2.242601567559177e-06\n",
      "Epoch 79, accuracy: 0.5531315974665728\n",
      "Epoch 80, iteration 0, loss: 2.3823038191039814e-06\n",
      "Epoch 80, accuracy: 0.5531315974665728\n",
      "Epoch 81, iteration 0, loss: 3.015592938027112e-06\n",
      "Epoch 81, accuracy: 0.5531315974665728\n",
      "Epoch 82, iteration 0, loss: 2.376716111029964e-06\n",
      "Epoch 82, accuracy: 0.553835327234342\n",
      "Epoch 83, iteration 0, loss: 3.2334367006114917e-06\n",
      "Epoch 83, accuracy: 0.5524278676988037\n",
      "Epoch 84, iteration 0, loss: 4.095870735909557e-06\n",
      "Epoch 84, accuracy: 0.5559465165376496\n",
      "Epoch 85, iteration 0, loss: 2.160657004424138e-06\n",
      "Epoch 85, accuracy: 0.5545390570021111\n",
      "Epoch 86, iteration 0, loss: 5.868731477676192e-06\n",
      "Epoch 86, accuracy: 0.5559465165376496\n",
      "Epoch 87, iteration 0, loss: 5.025081463827519e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_EPOCHS):\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (sentence, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     13\u001b[0m         sentence, label \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py:593\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mrecord_function\u001b[39;00m(_ContextDecorator):\n\u001b[1;32m    557\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager/function decorator that adds a label to a code block/function when running autograd profiler.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m    It is useful when tracing the code profile.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m \n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, args: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    model.train()\n",
    "\n",
    "    for i, (sentence, label) in enumerate(train_loader):\n",
    "        sentence, label = sentence.to(device), label.to(device)\n",
    "        model.zero_grad()\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "        loss = loss_function(tag_scores, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, iteration {i}, loss: {loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentence, label in test_loader:\n",
    "            sentence, label = sentence.to(device), label.to(device)\n",
    "\n",
    "            tag_scores = model(sentence)\n",
    "\n",
    "            _, predicted = torch.max(tag_scores, 1)\n",
    "\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch}, accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.5819845179451091\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence, label in test_loader:\n",
    "        sentence, label = sentence.to(device), label.to(device)\n",
    "\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        _, predicted = torch.max(tag_scores, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        true_positive += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"Final accuracy: {true_positive/total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
