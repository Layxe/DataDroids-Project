{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73ba900c66f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import spacy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data/tweeteval/emotion/train_text.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/train_labels.txt with 3257 lines\n",
      "Processing line 0/3257\n",
      "Processing line 500/3257\n",
      "Processing line 1000/3257\n",
      "Processing line 1500/3257\n",
      "Processing line 2000/3257\n",
      "Processing line 2500/3257\n",
      "Processing line 3000/3257\n",
      "Loading ./data/tweeteval/emotion/test_text.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n",
      "Loading ./data/tweeteval/emotion/test_labels.txt with 1421 lines\n",
      "Processing line 0/1421\n",
      "Processing line 500/1421\n",
      "Processing line 1000/1421\n"
     ]
    }
   ],
   "source": [
    "EMOTION_LABEL = {\n",
    "    \"anger\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"optimism\": 2,\n",
    "    \"sadness\": 3\n",
    "}\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, training):\n",
    "        spacy.prefer_gpu()\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.data = None\n",
    "\n",
    "        if training:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/train_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/train_labels.txt\"\n",
    "        else:\n",
    "            self.path_text   = \"./data/tweeteval/emotion/test_text.txt\"\n",
    "            self.path_labels = \"./data/tweeteval/emotion/test_labels.txt\"\n",
    "\n",
    "        self.data   = self._load_txt_file(self.path_text)\n",
    "        self.labels = self._load_txt_file(self.path_labels, perform_nlp=False)\n",
    "\n",
    "    def _load_txt_file(self, path, perform_nlp=True):\n",
    "\n",
    "        with open(path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            print(f\"Loading {path} with {len(lines)} lines\")\n",
    "\n",
    "            for i in range(len(lines)):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\"Processing line {i}/{len(lines)}\")\n",
    "\n",
    "                lines[i] = lines[i].strip()\n",
    "                lines[i] = lines[i].replace(\"\\n\", \"\")\n",
    "\n",
    "                if perform_nlp:\n",
    "                    lines[i] = self.nlp(lines[i])\n",
    "\n",
    "            return lines\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[index], int(self.labels[index]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = EmotionDataset(training=True)\n",
    "test_dataset  = EmotionDataset(training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256, vocab_size=5000, tagset_size=4):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "Epoch 0, iteration 100/3257, loss: 1.8801770210266113\n",
      "Epoch 0, iteration 200/3257, loss: 2.9997172355651855\n",
      "Epoch 0, iteration 300/3257, loss: 1.7394258975982666\n",
      "Epoch 0, iteration 400/3257, loss: 0.9320502877235413\n",
      "Epoch 0, iteration 500/3257, loss: 1.047295093536377\n",
      "Epoch 0, iteration 600/3257, loss: 1.1256669759750366\n",
      "Epoch 0, iteration 700/3257, loss: 1.338423252105713\n",
      "Epoch 0, iteration 800/3257, loss: 0.943281352519989\n",
      "Epoch 0, iteration 900/3257, loss: 0.6726950407028198\n",
      "Epoch 0, iteration 1000/3257, loss: 0.8542113304138184\n",
      "Epoch 0, iteration 1100/3257, loss: 1.6902644634246826\n",
      "Epoch 0, iteration 1200/3257, loss: 1.1624033451080322\n",
      "Epoch 0, iteration 1300/3257, loss: 0.918684720993042\n",
      "Epoch 0, iteration 1400/3257, loss: 1.2157502174377441\n",
      "Epoch 0, iteration 1500/3257, loss: 2.37111496925354\n",
      "Epoch 0, iteration 1600/3257, loss: 1.147670865058899\n",
      "Epoch 0, iteration 1700/3257, loss: 1.2347530126571655\n",
      "Epoch 0, iteration 1800/3257, loss: 0.688170850276947\n",
      "Epoch 0, iteration 1900/3257, loss: 1.4071862697601318\n",
      "Epoch 0, iteration 2000/3257, loss: 1.723367691040039\n",
      "Epoch 0, iteration 2100/3257, loss: 0.713304877281189\n",
      "Epoch 0, iteration 2200/3257, loss: 2.218092203140259\n",
      "Epoch 0, iteration 2300/3257, loss: 1.1301064491271973\n",
      "Epoch 0, iteration 2400/3257, loss: 1.2864867448806763\n",
      "Epoch 0, iteration 2500/3257, loss: 0.8451226949691772\n",
      "Epoch 0, iteration 2600/3257, loss: 1.726155400276184\n",
      "Epoch 0, iteration 2700/3257, loss: 0.7724762558937073\n",
      "Epoch 0, iteration 2800/3257, loss: 0.8246240615844727\n",
      "Epoch 0, iteration 2900/3257, loss: 1.707872748374939\n",
      "Epoch 0, iteration 3000/3257, loss: 1.0364255905151367\n",
      "Epoch 0, iteration 3100/3257, loss: 1.8625520467758179\n",
      "Epoch 0, iteration 3200/3257, loss: 2.083639144897461\n",
      "Epoch 1, iteration 100/3257, loss: 1.5378469228744507\n",
      "Epoch 1, iteration 200/3257, loss: 2.555269479751587\n",
      "Epoch 1, iteration 300/3257, loss: 1.675008773803711\n",
      "Epoch 1, iteration 400/3257, loss: 1.033816933631897\n",
      "Epoch 1, iteration 500/3257, loss: 0.9487144947052002\n",
      "Epoch 1, iteration 600/3257, loss: 1.449004888534546\n",
      "Epoch 1, iteration 700/3257, loss: 1.2972452640533447\n",
      "Epoch 1, iteration 800/3257, loss: 1.0204442739486694\n",
      "Epoch 1, iteration 900/3257, loss: 0.708669900894165\n",
      "Epoch 1, iteration 1000/3257, loss: 0.827634871006012\n",
      "Epoch 1, iteration 1100/3257, loss: 1.574678897857666\n",
      "Epoch 1, iteration 1200/3257, loss: 1.0424726009368896\n",
      "Epoch 1, iteration 1300/3257, loss: 0.8925812244415283\n",
      "Epoch 1, iteration 1400/3257, loss: 1.1770110130310059\n",
      "Epoch 1, iteration 1500/3257, loss: 2.5443248748779297\n",
      "Epoch 1, iteration 1600/3257, loss: 1.2789385318756104\n",
      "Epoch 1, iteration 1700/3257, loss: 1.2463006973266602\n",
      "Epoch 1, iteration 1800/3257, loss: 0.6961907148361206\n",
      "Epoch 1, iteration 1900/3257, loss: 1.447359561920166\n",
      "Epoch 1, iteration 2000/3257, loss: 1.6425964832305908\n",
      "Epoch 1, iteration 2100/3257, loss: 0.7086181640625\n",
      "Epoch 1, iteration 2200/3257, loss: 2.2182750701904297\n",
      "Epoch 1, iteration 2300/3257, loss: 1.098595142364502\n",
      "Epoch 1, iteration 2400/3257, loss: 1.2884504795074463\n",
      "Epoch 1, iteration 2500/3257, loss: 0.8927068114280701\n",
      "Epoch 1, iteration 2600/3257, loss: 1.7341464757919312\n",
      "Epoch 1, iteration 2700/3257, loss: 0.7725344300270081\n",
      "Epoch 1, iteration 2800/3257, loss: 0.8345241546630859\n",
      "Epoch 1, iteration 2900/3257, loss: 1.669628381729126\n",
      "Epoch 1, iteration 3000/3257, loss: 1.0425450801849365\n",
      "Epoch 1, iteration 3100/3257, loss: 1.8887122869491577\n",
      "Epoch 1, iteration 3200/3257, loss: 2.0812153816223145\n",
      "Epoch 2, iteration 100/3257, loss: 1.4994394779205322\n",
      "Epoch 2, iteration 200/3257, loss: 2.5221118927001953\n",
      "Epoch 2, iteration 300/3257, loss: 1.6824023723602295\n",
      "Epoch 2, iteration 400/3257, loss: 1.0333412885665894\n",
      "Epoch 2, iteration 500/3257, loss: 0.9488111138343811\n",
      "Epoch 2, iteration 600/3257, loss: 1.4485242366790771\n",
      "Epoch 2, iteration 700/3257, loss: 1.2972912788391113\n",
      "Epoch 2, iteration 800/3257, loss: 1.0203664302825928\n",
      "Epoch 2, iteration 900/3257, loss: 0.7086076736450195\n",
      "Epoch 2, iteration 1000/3257, loss: 0.8276823163032532\n",
      "Epoch 2, iteration 1100/3257, loss: 1.5748705863952637\n",
      "Epoch 2, iteration 1200/3257, loss: 1.0426338911056519\n",
      "Epoch 2, iteration 1300/3257, loss: 0.892656147480011\n",
      "Epoch 2, iteration 1400/3257, loss: 1.1770329475402832\n",
      "Epoch 2, iteration 1500/3257, loss: 2.5442922115325928\n",
      "Epoch 2, iteration 1600/3257, loss: 1.2788006067276\n",
      "Epoch 2, iteration 1700/3257, loss: 1.2462718486785889\n",
      "Epoch 2, iteration 1800/3257, loss: 0.696164608001709\n",
      "Epoch 2, iteration 1900/3257, loss: 1.4473203420639038\n",
      "Epoch 2, iteration 2000/3257, loss: 1.6426784992218018\n",
      "Epoch 2, iteration 2100/3257, loss: 0.7086023092269897\n",
      "Epoch 2, iteration 2200/3257, loss: 2.2183115482330322\n",
      "Epoch 2, iteration 2300/3257, loss: 1.098628282546997\n",
      "Epoch 2, iteration 2400/3257, loss: 1.288475751876831\n",
      "Epoch 2, iteration 2500/3257, loss: 0.8924384713172913\n",
      "Epoch 2, iteration 2600/3257, loss: 1.7341439723968506\n",
      "Epoch 2, iteration 2700/3257, loss: 0.772527813911438\n",
      "Epoch 2, iteration 2800/3257, loss: 0.8344970345497131\n",
      "Epoch 2, iteration 2900/3257, loss: 1.6696503162384033\n",
      "Epoch 2, iteration 3000/3257, loss: 1.042546272277832\n",
      "Epoch 2, iteration 3100/3257, loss: 1.888704538345337\n",
      "Epoch 2, iteration 3200/3257, loss: 2.081214666366577\n",
      "Epoch 3, iteration 100/3257, loss: 1.499452829360962\n",
      "Epoch 3, iteration 200/3257, loss: 2.5221171379089355\n",
      "Epoch 3, iteration 300/3257, loss: 1.6824078559875488\n",
      "Epoch 3, iteration 400/3257, loss: 1.033332109451294\n",
      "Epoch 3, iteration 500/3257, loss: 0.9488093256950378\n",
      "Epoch 3, iteration 600/3257, loss: 1.4485156536102295\n",
      "Epoch 3, iteration 700/3257, loss: 1.2972939014434814\n",
      "Epoch 3, iteration 800/3257, loss: 1.0203649997711182\n",
      "Epoch 3, iteration 900/3257, loss: 0.7086063027381897\n",
      "Epoch 3, iteration 1000/3257, loss: 0.827682375907898\n",
      "Epoch 3, iteration 1100/3257, loss: 1.5748765468597412\n",
      "Epoch 3, iteration 1200/3257, loss: 1.0426383018493652\n",
      "Epoch 3, iteration 1300/3257, loss: 0.8926592469215393\n",
      "Epoch 3, iteration 1400/3257, loss: 1.177033543586731\n",
      "Epoch 3, iteration 1500/3257, loss: 2.544292449951172\n",
      "Epoch 3, iteration 1600/3257, loss: 1.2787981033325195\n",
      "Epoch 3, iteration 1700/3257, loss: 1.2462713718414307\n",
      "Epoch 3, iteration 1800/3257, loss: 0.6961641311645508\n",
      "Epoch 3, iteration 1900/3257, loss: 1.4473199844360352\n",
      "Epoch 3, iteration 2000/3257, loss: 1.642679214477539\n",
      "Epoch 3, iteration 2100/3257, loss: 0.7086023092269897\n",
      "Epoch 3, iteration 2200/3257, loss: 2.2183096408843994\n",
      "Epoch 3, iteration 2300/3257, loss: 1.0986284017562866\n",
      "Epoch 3, iteration 2400/3257, loss: 1.2884771823883057\n",
      "Epoch 3, iteration 2500/3257, loss: 0.8924190998077393\n",
      "Epoch 3, iteration 2600/3257, loss: 1.734134554862976\n",
      "Epoch 3, iteration 2700/3257, loss: 0.7725276947021484\n",
      "Epoch 3, iteration 2800/3257, loss: 0.8344937562942505\n",
      "Epoch 3, iteration 2900/3257, loss: 1.6696501970291138\n",
      "Epoch 3, iteration 3000/3257, loss: 1.0425467491149902\n",
      "Epoch 3, iteration 3100/3257, loss: 1.8887051343917847\n",
      "Epoch 3, iteration 3200/3257, loss: 2.0812149047851562\n",
      "Epoch 4, iteration 100/3257, loss: 1.4994529485702515\n",
      "Epoch 4, iteration 200/3257, loss: 2.5221147537231445\n",
      "Epoch 4, iteration 300/3257, loss: 1.6824084520339966\n",
      "Epoch 4, iteration 400/3257, loss: 1.0333317518234253\n",
      "Epoch 4, iteration 500/3257, loss: 0.9488083124160767\n",
      "Epoch 4, iteration 600/3257, loss: 1.4485160112380981\n",
      "Epoch 4, iteration 700/3257, loss: 1.2972943782806396\n",
      "Epoch 4, iteration 800/3257, loss: 1.020365595817566\n",
      "Epoch 4, iteration 900/3257, loss: 0.7086063027381897\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Step 3. Run our forward pass.\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m tag_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_in\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Step 4. Compute the loss, gradients, and update the parameters by\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#  calling optimizer.step()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(tag_scores, targets)\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mLSTMTagger.forward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[1;32m     17\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(sentence)\n\u001b[0;32m---> 18\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     tag_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden2tag(lstm_out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(sentence), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     20\u001b[0m     tag_scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(tag_space, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Studium/Master-2/ki-lab-ss23/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "word_counts = {}\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in train_dataset:\n",
    "    for word in sent:\n",
    "        if word.text not in word_counts:\n",
    "            word_counts[word.text] = 1\n",
    "        else:\n",
    "            word_counts[word.text] += 1\n",
    "\n",
    "# Sort words by counts\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Assign ID to the most 5000 frequent words\n",
    "for word, _ in sorted_word_counts[:5000]:\n",
    "    word_to_ix[word] = len(word_to_ix) + 1\n",
    "\n",
    "print(len(word_to_ix))\n",
    "\n",
    "def get_idx(word):\n",
    "    if word in word_to_ix:\n",
    "        return word_to_ix[word]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [get_idx(w) for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(train_dataset[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    iteration_counter = 0\n",
    "\n",
    "    for sentence, label in train_dataset:\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor(label)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)[-1]\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration_counter += 1\n",
    "\n",
    "        if iteration_counter % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, iteration {iteration_counter}/{len(train_dataset)}, loss: {loss}\")\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(train_dataset[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
